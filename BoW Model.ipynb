{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8a019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "import warnings\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from time import sleep\n",
    "import sys\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33a51e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anonymized Message</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Strolling down memory lane in the name of new ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey yall. I'm going to tennis camp tommorow.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>... chocolate peanutbutter ice cream, yes plea...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOTHING leave me alone!!!!!!!!!!!!!!!!!</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Anonymized Message  Valence  Arousal\n",
       "0  Strolling down memory lane in the name of new ...      6.0      4.0\n",
       "1       Hey yall. I'm going to tennis camp tommorow.      5.0      3.0\n",
       "2                                                         6.0      2.0\n",
       "3  ... chocolate peanutbutter ice cream, yes plea...      6.0      5.0\n",
       "4            NOTHING leave me alone!!!!!!!!!!!!!!!!!      4.0      9.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['Anonymized Message', 'Valence', 'Arousal']\n",
    "dtypes = {'Anonymized Message':'str', 'Valence':'float', 'Arousal':'float'}\n",
    "data = pd.read_csv(\"dataset-fb-valence-arousal-anon.csv\", encoding='utf8', skiprows=[0], names=headers, dtype=dtypes, na_filter=False)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f260b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all words to lowercase and remove anything that isn't a letter or space\n",
    "data['Anonymized Message'] = data['Anonymized Message'].apply(lambda x: x.lower()) #transform text to lowercase\n",
    "data['Anonymized Message'] = data['Anonymized Message'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))\n",
    "data['Anonymized Message'] = data['Anonymized Message'].apply(lambda x: re.sub('[_\\\\/(){}\\[\\]\\|@,;]', '', x))\n",
    "\n",
    "# find and remove all stop words\n",
    "stop = stopwords.words('english')\n",
    "data['Anonymized Message'] = data['Anonymized Message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1674011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4266                                                     \n",
       "1573    saw paranormal activity yesterday awsome good ...\n",
       "5397                                                     \n",
       "2265                                                     \n",
       "3905                                                     \n",
       "4265    still need help horse stable need 6 nail 4 hor...\n",
       "Name: Anonymized Message, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # split X, y into training and testing \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['Anonymized Message'], data['Valence'], test_size=0.2, random_state=0)\n",
    "    X_train[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04737481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im', 'person', 'day', 'love', 'like', 'got', 'one', 'time', 'today', 'go']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "for comments in X_train:\n",
    "    for word in comments.split():\n",
    "        if word not in words_counts:\n",
    "            words_counts[word] = 1\n",
    "        words_counts[word] += 1\n",
    "        \n",
    "DICT_SIZE = 10000\n",
    "POPULAR_WORDS = sorted(words_counts, key=words_counts.get, reverse=True)[:DICT_SIZE]\n",
    "WORDS_TO_INDEX = {key: rank for rank, key in enumerate(POPULAR_WORDS, 0)}\n",
    "INDEX_TO_WORDS = {index:word for word, index in WORDS_TO_INDEX.items()}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "POPULAR_WORDS[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
